<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Snorkel</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <script async defer src="https://buttons.github.io/buttons.js"></script>

  </head>
  <body>
    <div class="wrapper">
      <header>
        <!--<h1>Snorkel</h1>-->
        <img src="figs/logo_01.png" width="100"/>
        <p>
          The System for Programmatically Building and Managing Training Data
        </p>

        <p class="view">
          <a href="https://github.com/HazyResearch/snorkel">View the Project on GitHub
        </p>

        <a class="github-button" href="https://github.com/HazyResearch/snorkel" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star HazyResearch/snorkel on GitHub">Star</a>

        <a class="github-button" href="https://github.com/HazyResearch/snorkel/fork" data-icon="octicon-repo-forked" data-size="large" data-show-count="true" aria-label="Fork HazyResearch/snorkel on GitHub">Fork</a>

        <p></p>
        <a class="twitter-timeline" data-width="300" data-height="400" href="https://twitter.com/ajratner/timelines/1143031904544813056?ref_src=twsrc%5Etfw">Recent Tweets</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
        <!--
        <h2>
            <a id="motivation" class="anchor" href="#motivation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>News</h2>

            <p>[6/4/19] Two papers using Snorkel (<a href="https://nature-research-under-consideration.nature.com/users/37265-nature-communications/posts/38921-weakly-supervised-classification-of-rare-aortic-valve-malformations-using-unlabeled-cardiac-mri-sequences">cardiac MRI imaging</a>, <a href="https://ai.stanford.edu/~kuleshov/papers/gwaskb-manuscript.pdf">GWAS studies</a>) accepted to Nature Communications</p>

            <p>[4/21/19] Two papers around <a href="https://arxiv.org/abs/1903.05844">learning weak supervision structure</a> and <a href="https://arxiv.org/abs/1803.06084">augmentation theory</a> accepted to ICML 2019</p>

            <p>[3/14/19] <a href="https://arxiv.org/abs/1812.00417">SIGMOD paper</a> and <a href="https://ai.googleblog.com/2019/03/harnessing-organizational-knowledge-for.html">Google AI blog</a> on Snorkel's usage at Google</p>

            <p><i>[See References for older]</i></p>
        -->

            <!-- MAX 3 ITEMS -->
            <!--
            <a onclick="news()">More... </a>
            <p></p>
            <div id="news"style="display:none">
                <p>[3/29/19] <a href="https://nature-research-under-consideration.nature.com/users/37265-nature-communications/posts/47370-cross-modal-data-programming-enables-rapid-medical-machine-learning">Manuscript</a>  on applying Snorkel to radiology and neurology applications posted</p>
                <p></p>

                <p>[3/21/19] Snorkel achieves <a href="https://dawn.cs.stanford.edu/2019/03/22/glue/">SOTA on GLUE Benchmark</a></p>
                <p></p>

                <p>[12/05/18] Snorkel shout-out in Kunle Olukotun's <a href="https://nips.cc/Conferences/2018/Schedule?showEvent=12469">keynote at NeurIPS 2018</a> </p>
                <p></p>

            </div>
          -->

      </header>
      <div>
          <p></p>
          <p></p>
        <section>


<h2>Snorkel: The System for Programmatically Building and Managing Training Data</h2>
<p>
  <b>
    Snorkel is a system for programmatically <i>building and managing</i> training datasets to rapidly and flexibly fuel machine learning models.
  </b>
</p>
<p>
  Today's state-of-the-art machine learning models are more powerful and easy to use than ever before- however, they require massive <i>training datasets</i>.
  Traditionally, these training datasets require slow and often prohibitively expensive manual labeling by domain experts.
  Instead, in Snorkel, <b>users write programmatic operations to <i>label, transform, and structure</i> training datasets</b> for machine learning, without needing to hand label <i>any</i> training data; Snorkel then uses modern, theoretically-grounded modeling techniques to clean and integrate the resulting training data.
</p>
<p>
  In a wide range of applications---from medical image monitoring to text information extraction to industrial deployments over web data---Snorkel provides a radically faster and more flexible to build machine learning applications, by letting users <b>programmatically build and manipulate training data</b> rather than label it by hand. Snorkel focuses on three key operations: <b>labeling data</b>, for example using heuristic rules or distant supervision techniques; <b>transforming data</b>, for example to perform data augmentation and express invariances in the data; and <b>slicing data</b> into different critical subsets.
</p>
<img align:center src="figs/fig_abstractions.png" alt="Snorkel" width=90% />

<p></p>
<hr>

<h2>Snorkel Users and Sponsors</h2>

<img align:right src="figs/logos.png" width="90%"  />
<p></p>

<h3>Recent News</h3>
<p>[6/4/19] Two papers using Snorkel (<a href="https://www.biorxiv.org/content/10.1101/339630v1">cardiac MRI imaging</a>, <a href="https://ai.stanford.edu/~kuleshov/papers/gwaskb-manuscript.pdf">GWAS studies</a>) accepted to Nature Communications</p>

<p>[4/21/19] Two papers around <a href="https://arxiv.org/abs/1903.05844">learning weak supervision structure</a> and <a href="https://arxiv.org/abs/1803.06084">augmentation theory</a> accepted to ICML 2019</p>

<p>[3/14/19] <a href="https://arxiv.org/abs/1812.00417">SIGMOD paper</a> and <a href="https://ai.googleblog.com/2019/03/harnessing-organizational-knowledge-for.html">Google AI blog</a> on Snorkel's usage at Google</p>

<p><i>[See References for more]</i></p>

<h3>Snorkel Highlights</h3>
<p>
    Snorkel has been used in production applications at places like <b><font color="6699CC"><a href="https://ai.googleblog.com/2019/03/harnessing-organizational-knowledge-for.html">Google</a></font></b>, <b><font color="6699CC"><a href="https://arxiv.org/pdf/1812.06176.pdf">IBM</a></font></b>,   and <b><font color="6699CC"><a href="https://ajratner.github.io/assets/papers/Osprey_DEEM.pdf">Intel</a></font></b>, and has recently been used to achieve state-of-the-art performance on the <b><font color="6699CC"><a href="https://dawn.cs.stanford.edu/2019/03/22/glue/">GLUE</a></font></b> and
    <b><font color="6699CC"><a href="http://hazyresearch.github.io/snorkel/blog/superglue.html">SuperGLUE language understanding benchmarks</a></font></b>!

</p>
<img align:center src="figs/nature.png" alt="Snorkel" width=100% />

<p>
    Snorkel has been used extensively in medical settings, highlighted by two recent Nature Communication publications around <b><font color="6699CC"><a href="https://ai.stanford.edu/~kuleshov/papers/gwaskb-manuscript.pdf">automated GWAS curation</a></font></b> and <b><font color="6699CC"><a href="https://nature-research-under-consideration.nature.com/users/37265-nature-communications/posts/38921-weakly-supervised-classification-of-rare-aortic-valve-malformations-using-unlabeled-cardiac-mri-sequences">cardiac MRI classification</a></font></b>, in various <b><font color="6699CC"><a href="https://arxiv.org/abs/1903.11101">radiology and neurological monitoring settings</a></font></b> where it has been used to replace person months of hand-labeling, and to extract information from electronic health record (EHR) data, offering a scalable solution for national <b><font color="6699CC"><a href="https://arxiv.org/abs/1904.07640">medical device surveillance</a></font></b>.
</p>
<p>
  For more use cases, as well as the various academic publications explaining the technical work underlying Snorkel, see the References section below.
</p>


<p></p>
<hr>
<h2>Snorkel in More Detail</h2>

<p>
    Snorkel is a general framework that supports several <b><font color="6699CC"><a href="http://hazyresearch.github.io/snorkel/blog/weak_supervision.html">weak supervision techniques</a></font></b> and allows domain experts to encode their knowledge programmatically to provide supervision through the following operations:
</p>

<h3>
    <a id="users" class="anchor" href="#users" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Labeling
</h3>

<img align:center src="figs/lfs.png" alt="Snorkel" width=100% />
<p></p>

<p>
    Rather than requiring users to label training data points by hand, Snorkel takes as input <b>labeling functions (LFs)</b>, functions that heuristically or noisily label some subset of the training examples. Snorkel then models the quality and correlations of these LFs using novel, theoretically-grounded statistical modeling techniques. Read more here:
    <ul>
        <li>
            <a href="http://hazyresearch.github.io/snorkel/blog/weak_supervision.html">Blog on using Labeling Functions to Efficiently Label Training Data</a>
        </li>

        <li>
            <a href="https://arxiv.org/abs/1605.07723">NeurIPS'16 Paper on Data Programming</a>
        </li>

        <li>
            <a href="https://arxiv.org/abs/1711.10160">VLDB'18 Paper on Snorkel for Rapidly Labeling Training Data</a>
        </li>

        <li>
            <a href="https://arxiv.org/abs/1810.02840">AAAI'19 Paper on Core Labeling Function Modeling Method and Theory</a>
        </li>
      </ul>
</p>


<h3>
    <a id="users" class="anchor" href="#users" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Tranforming
</h3>
<img align:center src="figs/tfs.png" alt="Snorkel" width=100% />
<p></p>

<p>
    Snorkel also lets users write <b>transformation functions (TFs)</b> to heuristically generate new, modified training examples by transforming existing ones---a strategy often referred to as <i>data augmentation</i>. Rather than requiring users to tune these data augmentation or transformation strategies by hand, Snorkel learns compositions of transformations across various domain-specific tasks to optimize for a representative training set. Read more here:
    <ul>
        <li>
          <a href="https://hazyresearch.github.io/snorkel/blog/tanda.html">Blog on using Transformation Functions to Augment Training Set</a>
        </li>

        <li>
          <a href="https://arxiv.org/abs/1709.01643">NeurIPS'17 Paper on Learning Data Augmentation Policies</a>
        </li>

        <li>
            <a href="https://arxiv.org/abs/1803.06084">ICML'19 Paper on Theory of Data Augmentation</a>
          </li>

        <li>
            <a href="https://ai.googleblog.com/2018/06/improving-deep-learning-performance.html">Follow Up Work: Google's AutoAugment to Learn Augmentation Policies</a>
          </li>
      </ul>
</p>

<h3>Slicing</h3>

<img align:center src="figs/sfs.png" alt="Snorkel" width=100% />
<p></p>
<p>
    Finally, Snorkel also lets users write <b>slicing functions (SFs)</b> to heuristically identify subsets of the data the model should particularly care about, e.g. have extra representative capacity for, due to their difficulty and/or importance. It models slices in the style of multi-task learning and an attention-mechanism is then learned over these heads. Read more here:
    <ul>
        <li>
            <a href="http://hazyresearch.github.io/snorkel/blog/superglue.html">Blog on using Slicing Functions to achieves SOTA Performance</a>
        </li>
      </ul>
</p>

<p>
    Snorkel can also operate over other forms of weak supervision like crowdsourcing by modeling individual workers as labeling functions. To properly take advantage of all supervision signal available, Snorkel can takes advantage of multi-task learning and transfer learning, moving towards <b><font color="6699CC"><a href="https://ajratner.github.io/assets/papers/software_2_mmt_vision.pdf">massive multi-task learning</a></font></b> to facilitate incorporating diverse and varying granularities of supervision at a large scale.
</p>

<hr>
<h2>References</h2>


<h3>Blogs and Tutorials</h3>
  <ul>
      <li>
        [3/23/2019] <a href="https://dawn.cs.stanford.edu/2019/03/22/glue/">Massive Multi-Task Learning with Snorkel MeTaL: Bringing More Supervision to Bear</a>
      </li>
      <li>
        [2/4/2019] <a href="https://hazyresearch.github.io/snorkel/blog/mtl_systems.html">Emerging Topics in Multi-Task Learning Systems</a>
      </li>
      <li>
        [12/4/2018] <a href="http://hazyresearch.github.io/snorkel/blog/s2_programming.html">Software 2.0 and the Paradigm Shift in Programming ML Systems</a>
      </li>
      <li>
        [06/21/2018] <a href="http://dawn.cs.stanford.edu/2018/06/21/debugging/">Systematically Debugging Training Data for Software 2.0</a>
      </li>
      <li>
        [11/30/2017] <a href="https://hazyresearch.github.io/snorkel/blog/snorkel_programming_training_data.html">Weak Supervision: The New Programming Language for Software 2.0</a>
      </li>
      <li>
        [09/20/2017] <a href="http://dawn.cs.stanford.edu/2017/09/14/coral/">Exploiting Building Blocks of Data to Efficiently Create Training Sets</a>
      </li>
      <li>
          [08/10/2017] <a href="https://hazyresearch.github.io/snorkel/blog/tanda.html">Learning to Compose Domain-Specific Transformations for Data Augmentation</a> [<a href="https://github.com/HazyResearch/tanda">Repo</a>]
        </li>
        <li>
          [07/12/2017] <a href="https://hazyresearch.github.io/snorkel/blog/ws_blog_post.html">Weak Supervision: The New Programming Paradigm for Machine Learning</a>
        </li>
        <li>
          [06/05/2017] <a href="https://hazyresearch.github.io/snorkel/blog/snark.html">Scaling Up Snorkel with Spark</a> [<a href="https://github.com/HazyResearch/snorkel/blob/master/tutorials/snark/Snark%20Tutorial.ipynb">Tutorial</a>]
        </li>
        <li>
          [05/08/2017] <a href="https://hazyresearch.github.io/snorkel/blog/holoclean.html">HoloClean: Weakly Supervised Data Repairing</a>
        </li>
        <li>
          [04/17/2017] <a href="https://hazyresearch.github.io/snorkel/blog/structure_learning.html">Structure Learning: Are Your Sources Only Telling You What You Want to Hear?</a> [<a href="https://github.com/HazyResearch/snorkel/blob/master/tutorials/advanced/Structure_Learning.ipynb">Tutorial</a>]
        </li>
        <li>
          [03/21/2017] <a href="https://hazyresearch.github.io/snorkel/blog/babble_labble.html">Babble Labble: Learning from Natural Language Explanations</a>
        </li>
        <li>
          [03/16/2017] <a href="https://hazyresearch.github.io/snorkel/blog/fonduer.html">Fonduer: Knowledge Base Construction from Richly Formatted Data</a>
        </li>
        <li>
          [12/15/2016] <a href="http://hazyresearch.github.io/snorkel/blog/dp_with_tf_blog_post.html">Data Programming + TensorFlow Tutorial</a> (<a href="http://hazyresearch.github.io/snorkel/blog/DataProgrammingTensorFlow-v2.ipynb">notebook version</a>)
        </li>
        <li>
          [11/24/2016] <a href="http://hazyresearch.github.io/snorkel/blog/slimfast.html">SLiMFast: Assessing the Reliability of Data</a>
        </li>
        <li>
          [10/24/2016] <a href="http://hazyresearch.github.io/snorkel/blog/socratic_learning.html">Socratic Learning: Debugging ML Models</a>
        </li>
        <li>
          [9/19/2016] <a href="http://hazyresearch.github.io/snorkel/blog/weak_supervision.html">Data Programming: ML with Weak Supervision</a> [<a href="https://github.com/HazyResearch/snorkel/tree/master/tutorials/intro">Tutorial</a>]
        </li>
        </ul>

<p></p>
<h3>Papers and Pre-Prints</h3>
    <ul>
        <li>
          <b><a href="https://arxiv.org/abs/1711.10160">Snorkel: Rapid Training Data Creation with Weak Supervision</a> (VLDB 2018)</b>
        </li>
      <li>
        <b><a href="https://arxiv.org/abs/1605.07723">Data Programming: Creating Large Training Sets, Quickly</a> (NeurIPS 2016)</b>
      </li>
      <li>
          <b><a href="https://cs.stanford.edu/~chrismre/papers/Chris_Re-KDD.pdf">Snorkel and the Software 2.0 vision</a> (KDD 2018)</b>
      </li>
        <b><a href="https://arxiv.org/abs/1703.00854">Learning the Structure of Generative Models without Labeled Data</a> (ICML 2017)</b>
      </li>
      <li>
        <b><a href="https://arxiv.org/pdf/1903.05844.pdf">Learning Dependency Structures for Weak Supervision Models</a> (Arxiv 2019)</b>
      </li>
      <li>
        <b><a href="https://arxiv.org/abs/1810.02840">Training Complex Models with Multi-Task Weak Supervision</a> (AAAI 2019)</b>
      </li>
      <li>
        <b><a href="https://ajratner.github.io/assets/papers/software_2_mmt_vision.pdf">The Role of Massively Multi-Task and Weak Supervision in Software 2.0</a> (CIDR 2019)</b>
      </li>
      <li>
        <a href="pdfs/snorkel_demo.pdf">Snorkel: Fast Training Set Generation for Information Extraction</a> (SIGMOD DEMO 2017)
      </li>
      <li>
        <a href="https://arxiv.org/abs/1709.02477">Inferring Generative Model Structure with Static Analysis</a> (NeurIPS 2017)
      </li>
      <li>
        <a href="https://arxiv.org/abs/1805.03818">Training Classifiers with Natural Language Explanations</a> (ACL 2018)
      </li>
      <li>
        <a href="http://cs.stanford.edu/people/chrismre/papers/DDL_HILDA_2016.pdf">Data Programming with DDLite: Putting Humans in a Different Part of the Loop</a> (HILDA @ SIGMOD 2016; note Snorkel was previously <em>DDLite</em>)
      </li>
      <li>
        <a href="https://arxiv.org/abs/1610.08123">Socratic Learning: Correcting Misspecified Generative Models using Discriminative Models</a>
      </li>
      <li>
        <a href="https://arxiv.org/abs/1703.05028" target="_blank">Fonduer: Knowledge Base Construction from Richly Formatted Data</a> (SIGMOD 2018)
      </li>
      <li>
          <a href="https://arxiv.org/abs/1709.01643">Learning to Compose Domain-Specific Transformations for Data Augmentation</a> (NeurIPS 2017)
        </li>
        <li>
          <a href="https://arxiv.org/abs/1709.02605">Gaussian Quadrature for Kernel Features</a> (NeurIPS 2017)
        </li>
      </ul>

<p></p>
<h3>Snorkel Use Cases</h3>
<ul>
<li>
    Conversational agents at IBM: <a href="https://arxiv.org/pdf/1812.06176.pdf">Bootstrapping Conversational Agents With Weak Supervision (AAAI 2019)</a>
  </li>
  <li>
    Web content & event classification at Google: <a href="https://arxiv.org/abs/1812.00417">Snorkel DryBell: A Case Study in Deploying Weak Supervision at Industrial Scale (SIGMOD Industry 2019)</a>, and <a href="https://ai.googleblog.com/2019/03/harnessing-organizational-knowledge-for.html">Google AI blog post</a>
  </li>
  <li>
    Business intelligence at Intel: <a href="https://ajratner.github.io/assets/papers/Osprey_DEEM.pdf">Osprey: Non-Programmer Weak Supervision of Imbalanced Extraction Problems (SIGMOD DEEM 2019)</a>
  </li>
  <li>
    Anti-semitic tweet classification w/ Snorkel + transfer learning: <a href="https://t.co/h0zGQwDD59">A Technique for Building NLP Classifiers Efficiently with Transfer Learning and Weak Supervision (Blog post 2019)</a>
  </li>
  <li>
      Clinical text classification: <a href="https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-018-0723-6">A clinical text classification paradigm using weak supervision and deep representation (BMC MIDM 2019)</a>
    </li>
  <li>
    Social media text mining: <a href="https://ieeexplore.ieee.org/abstract/document/8609589/authors#authors">Deep Text Mining of Instagram Data without Strong Supervision (ICWI 2018)</a>
  </li>
  <li>
    Cardiac MRI classification with Stanford Medicine: <a href="https://www.biorxiv.org/content/biorxiv/early/2018/06/05/339630.full.pdf">Weakly supervised classification of rare aortic valve malformations using unlabeled cardiac MRI sequences (BioArxiv 2018)</a>
  </li>
  <li>
    Catching cheating at Chegg (<a href="https://www.edsurge.com/news/2018-03-01-cheating-on-chegg-maybe-not-on-its-tutoring-platform">Article</a>)
  </li>
  <li>
    Medical image triaging at Stanford Radiology: <a href="#">Cross-Modal Data Programming for Medical Images (NeurIPS ML4H 2017)</a>
  </li>
  <li>
    GWAS KBC with Stanford Genomics: <a href="https://ajratner.github.io/assets/papers/neurips2016-healthcare.pdf">A Machine-Compiled Database of Genome-Wide Association Studies (NeurIPS ML4H 2016)</a>
  </li>
</ul>

</section>
<!-- <footer>

  <p>
    <small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small>
  </p>
</footer> -->
</div>
<script src="javascripts/scale.fix.js"></script>

<script>
    function news() {
      var x = document.getElementById("news");
      if (x.style.display === "none") {
        x.style.display = "block";
      } else {
        x.style.display = "none";
      }
    }
    </script>

  </body>
</html>
