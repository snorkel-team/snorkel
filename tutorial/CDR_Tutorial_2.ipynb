{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chemical-Disease Relation (CDR) Tutorial\n",
    "\n",
    "In this example, we'll be writing an application to extract *mentions of* **chemical-induced-disease relationships** from Pubmed abstracts, as per the [BioCreative CDR Challenge](http://www.biocreative.org/resources/corpora/biocreative-v-cdr-corpus/).  At core, we will be constructing a model to classify _candidate_ CDR mentions as either true or false."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: `Candidate` Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "\n",
    "# Note: We run automated tests on this tutorial to make sure that it is always up to date! \n",
    "# However, certain interactive components cannot currently be tested automatically, and will \n",
    "# be skipped with if-then statements using the variable below\n",
    "AUTOMATED_TESTING = os.environ.get('TESTING') is not None\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the `Corpus`\n",
    "\n",
    "First, we will load the `Corpus` that we preprocessed in Part I:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.models import Corpus\n",
    "\n",
    "corpus = session.query(Corpus).filter(Corpus.name == 'CDR Training').one()\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we collect each `Sentence` in the `Corpus` into a `set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = set()\n",
    "for document in corpus:\n",
    "    for sentence in document.sentences:\n",
    "        sentences.add(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a `Candidate` schema\n",
    "We now define the schema of the relation mention we want to extract (which is also the schema of the candidates).  This must be a subclass of `Candidate`; we can manually define this class, or use a helper function (similar in spirit to the `collections.namedtuple` function).\n",
    "\n",
    "Here we'll define a _chemical disease relation mention candidate class_ which is composed of two named contexts, corresponding to a _chemical mention_ and a _disease mention_.  Note that this function will create the table if it does not exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "ChemicalDisease = candidate_subclass('ChemicalDisease', ['chemical', 'disease'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a basic `CandidateExtractor`\n",
    "\n",
    "Next, we'll write a basic function to extract **candidate relations mentions** from the corpus.  For this first attempt, we'll just write a function that checks for matches against several dictionaries at the _entity mention level_--i.e. looking for candidate chemical and disease mentions--and then considering any co-occuring pairs in the same sentence as candidate relation mentions.\n",
    "\n",
    "We'll use some precomputed disease and chemical dictionaries (see `tutorial/data/dicts/compile_dictionaries.py` for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the dictionaries\n",
    "ROOT = '%s/tutorial/data/dicts/' % os.environ['SNORKELHOME']\n",
    "disease_phrases   = open(ROOT + 'disease_phrases.txt', 'rb').read().strip().split('\\n')\n",
    "disease_acronyms  = open(ROOT + 'disease_acronyms.txt', 'rb').read().strip().split('\\n')\n",
    "chemical_phrases  = open(ROOT + 'chemical_phrases.txt', 'rb').read().strip().split('\\n')\n",
    "chemical_acronyms = open(ROOT + 'chemical_acronyms.txt', 'rb').read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We turn the dictionaries into a candidate extractor in three steps.\n",
    "\n",
    "First, we define a child context space for our sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.candidates import Ngrams\n",
    "\n",
    "ngrams = Ngrams(n_max=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we define matchers to filter the child contexts based on the dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.matchers import DictionaryMatch, Union\n",
    "\n",
    "# Define a matcher for diseases\n",
    "disease_matcher = Union(\n",
    "    DictionaryMatch(d=disease_phrases, ignore_case=True),\n",
    "    DictionaryMatch(d=disease_acronyms, ignore_case=False),\n",
    "    longest_match_only=True\n",
    ")\n",
    "\n",
    "# Define a matcher for chemicals\n",
    "chem_matcher = Union(\n",
    "    DictionaryMatch(d=chemical_phrases, ignore_case=True),\n",
    "    DictionaryMatch(d=chemical_acronyms, ignore_case=False),\n",
    "    longest_match_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, we combine the candidate class, child context space, and matchers into an extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.candidates import CandidateExtractor\n",
    "\n",
    "ce = CandidateExtractor(ChemicalDisease, [ngrams, ngrams], [chem_matcher, disease_matcher])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the `CandidateExtractor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the `CandidateExtractor` by calling extract with the contexts to extract from, a name for the `CandidateSet` that will contain the results, and the current session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%time c = ce.extract(sentences, 'CDR Training Candidates', session)\n",
    "print \"Number of candidates:\", len(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the extracted candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session.add(c)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reloading the candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.models import CandidateSet\n",
    "c = session.query(CandidateSet).filter(CandidateSet.name == 'CDR Training Candidates').one()\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the `Viewer` to inspect candidates\n",
    "\n",
    "Next, we'll use the `Viewer` class--here, specifically, the `SentenceNgramViewer`--to inspect the data.\n",
    "\n",
    "It is important to note, our goal here is to **maximize the recall of true candidates** extracted, **not** to extract _only_ the correct candidates. Learning to distinguish true candidates from false candidates is covered in Tutorial 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "if not AUTOMATED_TESTING:\n",
    "    sv = SentenceNgramViewer(c[:300], session, annotator_name=\"Tutorial Part 2 User\")\n",
    "else:\n",
    "    sv = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we instantiate and render the `Viewer` object; note we're being a bit sloppy, passing in _all_ the candidates and gold labels, but the `Viewer` object will take care of indexing them by sentence, and will only render the sentences we pass in:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, now we render the `Viewer`. <span style=\"color:red\">Red</span> denotes the first argument (chemical) and <span style=\"color:blue\">blue</span> denotes the second (disease)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can **navigate using the provided buttons**, or **using the keyboard (hover over buttons to see controls)**, highlight candidates (even if they overlap), and also **apply binary labels** (more on where to use this later!).  In particular, note that **the Viewer is synced dynamically with the notebook**, so that we can for example get the `Candidate` that is currently selected. Try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not AUTOMATED_TESTING:\n",
    "    print sv.get_selected()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composing fancier `CandidateExtractor`s\n",
    "\n",
    "We can additionally try to increase our candidate recall using more of the `Matcher` operators and their functionalities.  For example we can turn on **Porter stemming** in our dictionary matcher; Porter stemming is an aggressive rules-based method for normalizing word endings.  Another thing we can do is to allow for candidates to be subspans of each other by setting `longest_match_only=False` (note that this must be done in the outermost `Matcher`).\n",
    "\n",
    "We can also use the `Concat` and `RegexMatch` operators to find candidate mentions composed of an _adjective followed by a term matching our diseases dictionary_.  Note in particular that we set `left_required=False` so that exact matches to our dictionary (with no adjective prepended) will still work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.matchers import Concat, RegexMatchEach\n",
    "\n",
    "disease_matcher = Union(\n",
    "    Concat(\n",
    "        RegexMatchEach(rgx=r'JJ*', attrib='poses'),\n",
    "        DictionaryMatch(d=disease_phrases, stemmer='porter'),\n",
    "        left_required=False),\n",
    "    DictionaryMatch(d=disease_acronyms, ignore_case=False),\n",
    "    longest_match_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a new `CandidateExtractor` with the new `Matcher` for diesease, keeping everything else the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ce_fancy = CandidateExtractor(ChemicalDisease, [ngrams, ngrams], [chem_matcher, disease_matcher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time c = ce_fancy.extract(sentences, 'CDR Training Candidates -- Fancy', session)\n",
    "print len(c)\n",
    "session.add(c)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates a pretty large candidate set, however, so for the rest of the tutorial we'll use our initial simpler set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeating for development and test corpora\n",
    "We will rerun the same operations for the other two CDR corpora: development and test. All we do for each is load in the `Corpus` object, collect the `Sentence` objects, and run them through the `CandidateExtractor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for corpus_name in ['CDR Development', 'CDR Test']:\n",
    "    corpus = session.query(Corpus).filter(Corpus.name == corpus_name).one()\n",
    "    sentences = set()\n",
    "    for document in corpus:\n",
    "        for sentence in document.sentences:\n",
    "            sentences.add(sentence)\n",
    "    \n",
    "    %time c = ce.extract(sentences, corpus_name + ' Candidates', session)\n",
    "    session.add(c)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, in Part 3, we will annotate some candidates with labels so that we can evaluate performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "widgets": {
   "state": {
    "64248d182be14781b4b8a13ed2284b2d": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
