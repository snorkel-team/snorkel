{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disease Tagging Tutorial\n",
    "\n",
    "In this example, we'll be writing an application to extract *mentions of* diseases from Pubmed abstracts, using annotations from the [BioCreative CDR Challenge](http://www.biocreative.org/resources/corpora/biocreative-v-cdr-corpus/).  This tutorial, which has 5 parts, walks through the process of constructing a model to classify _candidate_ disease mentions as either true (i.e., that it is truly a mention of a disease) or false."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: `Candidate` Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the `Corpus`\n",
    "\n",
    "First, we will load the `Corpus` that we preprocessed in Part I:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.models import Corpus\n",
    "\n",
    "corpus = session.query(Corpus).filter(Corpus.name == 'CDR Training').one()\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we collect each `Sentence` in the `Corpus` into a `set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = set()\n",
    "for document in corpus:\n",
    "    for sentence in document.sentences:\n",
    "        sentences.add(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a `Candidate` schema\n",
    "We now define the schema of the relation mention we want to extract (which is also the schema of the candidates).  This must be a subclass of `Candidate`, and we define it using a helper function.\n",
    "\n",
    "Here we'll define a unary _disease relation mention_ which encapsulates a `Span` of text.  Note that this function will create the table in the database backend if it does not exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "Disease = candidate_subclass('Disease', ['disease'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a basic `CandidateExtractor`\n",
    "\n",
    "Next, we'll write a basic function to extract **candidate relations mentions** from the corpus.  For this first attempt, we'll just write a function that checks for matches against several dictionaries at the _entity mention level_--i.e. looking for candidate chemical and disease mentions--and then considering any co-occuring pairs in the same sentence as candidate relation mentions.\n",
    "\n",
    "We'll use some precomputed disease and chemical dictionaries (see `tutorial/data/dicts/compile_dictionaries.py` for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the dictionaries\n",
    "import pandas as pd\n",
    "ROOT = 'data/dicts/'\n",
    "diseases   = set(pd.read_csv(ROOT + 'disease_names.csv', header=None, index_col=0, encoding='utf-8').dropna()[1])\n",
    "abbrvs  = set(pd.read_csv(ROOT + 'disease_abbrvs.csv', header=None, index_col=0, encoding='utf-8').dropna()[1])\n",
    "body_parts  = set(pd.read_csv(ROOT + 'body_parts.csv', header=None, index_col=0, encoding='utf-8').dropna()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We turn the dictionaries into a candidate extractor in three steps.\n",
    "\n",
    "First, we define a child context space for our sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.candidates import Ngrams\n",
    "\n",
    "ngrams = Ngrams(n_max=8, split_tokens=['-', '/'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we define matchers to filter the child contexts based on the dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.matchers import DictionaryMatch, Concat\n",
    "\n",
    "#\n",
    "# DICTIONARIES\n",
    "#\n",
    "longest_match_only = True\n",
    "stemmer='porter'\n",
    "dict_diseases = DictionaryMatch(d=diseases, ignore_case=True, \n",
    "                                longest_match_only=longest_match_only)\n",
    "dict_abbrvs = DictionaryMatch(d=abbrvs, ignore_case=False, \n",
    "                              longest_match_only=longest_match_only)\n",
    "\n",
    "#\n",
    "#  STEM WORDS (diseases + abbrvs + generic disease terms)\n",
    "#\n",
    "keep = [\"disease\", \"diseases\", \"syndrome\", \"syndromes\", \"disorder\", \n",
    "        \"disorders\", \"damage\", \"infection\", \"bleeding\"]\n",
    "stems = diseases | abbrvs | set(keep)\n",
    "disease_stems = DictionaryMatch(d=stems, ignore_case=True, \n",
    "                                longest_match_only=longest_match_only)\n",
    "\n",
    "#\n",
    "# TYPED DISEASES\n",
    "# \n",
    "type_names = ['type', 'class', 'factor']\n",
    "type_nums = ['i', 'ii', 'iii', 'vi', 'v', 'vi', '1a', 'iid', 'a', 'b', 'c', 'd'] \n",
    "type_nums += map(unicode,range(1,10))\n",
    "\n",
    "types = Concat(DictionaryMatch(d=type_names),\n",
    "               DictionaryMatch(d=type_nums))\n",
    "\n",
    "disease_types_left = Concat(types, disease_stems)\n",
    "disease_types_right = Concat(disease_stems, types)\n",
    "\n",
    "# # \n",
    "# # DISEASES WITH BODY PARTS\n",
    "# #\n",
    "# #\n",
    "disease_pattern = [\"disease\", \"diseases\", \"syndrome\", \"syndromes\", \"disorder\", \"disorders\", \"damage\", \"infection\", \n",
    "       \"lesion\", \"lesions\", \"impairment\", \"impairments\", \"failure\", \"failures\", \"occlusion\", \"occlusions\", \n",
    "       \"dysfunction\", \"dysfunctions\", \"toxicity\", \"injury\", \"carcinoma\", \"carcinomas\", \"thrombosis\", \"cancer\", \n",
    "       \"cancers\", \"block\", \"pain\"]\n",
    "\n",
    "timestamp = [\"end-stage\", \"acute\", \"chronic\", \"congestive\"]\n",
    "\n",
    "conjunction = [\"and\", \"or\", \"and/or\"]\n",
    "\n",
    "stemmer='porter'\n",
    "body_disease = Concat(Concat(DictionaryMatch(d=body_parts, longest_match_only=longest_match_only, stemmer=stemmer), \n",
    "                             DictionaryMatch(d=conjunction, longest_match_only=longest_match_only)), \n",
    "                      Concat(DictionaryMatch(d=timestamp, longest_match_only=longest_match_only), \n",
    "                             Concat(DictionaryMatch(d=body_parts, longest_match_only=longest_match_only, stemmer=stemmer), \n",
    "                                    DictionaryMatch(d=disease_pattern, longest_match_only=longest_match_only, stemmer=stemmer)), left_required=False), left_required=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a `Union` of the `Matcher` objects we have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.matchers import Union\n",
    "\n",
    "disease_matcher = Union(disease_types_left, disease_types_right, dict_diseases, dict_abbrvs, body_disease,\n",
    "                        longest_match_only=longest_match_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, we combine the candidate class, child context space, and matcher into an extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.candidates import CandidateExtractor\n",
    "\n",
    "ce = CandidateExtractor(Disease, [ngrams], [disease_matcher])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the `CandidateExtractor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the `CandidateExtractor` by calling extract with the contexts to extract from, a name for the `CandidateSet` that will contain the results, and the current session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%time c = ce.extract(sentences, 'CDR Training Candidates', session)\n",
    "print \"Number of candidates:\", len(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the extracted candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session.add(c)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reloading the candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.models import CandidateSet\n",
    "c = session.query(CandidateSet).filter(CandidateSet.name == 'CDR Training Candidates').one()\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the `Viewer` to inspect candidates\n",
    "\n",
    "Next, we'll use the `Viewer` class--here, specifically, the `SentenceNgramViewer`--to inspect the data.\n",
    "\n",
    "It is important to note, our goal here is to **maximize the recall of true candidates** extracted, **not** to extract _only_ the correct candidates. Learning to distinguish true candidates from false candidates is covered in Tutorial 4.\n",
    "\n",
    "First, we instantiate the `Viewer` object, which groups the input `Candidate` objects by `Sentence`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    sv = SentenceNgramViewer(c[:300], session, annotator_name=\"Tutorial Part 2 User\")\n",
    "else:\n",
    "    sv = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we render the `Viewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can **navigate using the provided buttons**, or **using the keyboard (hover over buttons to see controls)**, highlight candidates (even if they overlap), and also **apply binary labels** (more on where to use this later!).  In particular, note that **the Viewer is synced dynamically with the notebook**, so that we can for example get the `Candidate` that is currently selected. Try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not AUTOMATED_TESTING:\n",
    "    print sv.get_selected()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeating for development and test corpora\n",
    "We will rerun the same operations for the other two CDR corpora: development and test. All we do for each is load in the `Corpus` object, collect the `Sentence` objects, and run them through the `CandidateExtractor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for corpus_name in ['CDR Development', 'CDR Test']:\n",
    "    corpus = session.query(Corpus).filter(Corpus.name == corpus_name).one()\n",
    "    sentences = set()\n",
    "    for document in corpus:\n",
    "        for sentence in document.sentences:\n",
    "            sentences.add(sentence)\n",
    "    \n",
    "    %time c = ce.extract(sentences, corpus_name + ' Candidates', session)\n",
    "    session.add(c)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, in Part 3, we will annotate some candidates with labels so that we can evaluate performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
