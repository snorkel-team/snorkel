{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro. to Snorkel: Extracting Spouse Relations from the News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to extract _candidates_ from our corpus. A `candidate` in Snorkel are the objects for which we want to make predictions. In this case, the candidates are pairs of people mentioned in sentences, and our task is to predict which pairs are described as married in the associated text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: `Candidate` Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "\n",
    "# TO USE A DATABASE OTHER THAN SQLITE, USE THIS LINE\n",
    "# Note that this is necessary for parallel execution amongst other things...\n",
    "# os.environ['SNORKELDB'] = 'postgres:///snorkel-intro'\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a `Candidate` schema\n",
    "We now define the schema of the relation mention we want to extract (which is also the schema of the candidates).  This must be a subclass of `Candidate`, and we define it using a helper function. Here we'll define a binary _spouse relation mention_ which connects two `Span` objects of text.  Note that this function will create the table in the database backend if it does not exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "Spouse = candidate_subclass('Spouse', ['person1', 'person2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a basic `CandidateExtractor`\n",
    "\n",
    "Next, we'll write a basic function to extract **candidate spouse relation mentions** from the corpus.  The `SentenceParser` we used in Part I is built on [CoreNLP](http://stanfordnlp.github.io/CoreNLP/), which performs _named entity recognition_ for us.\n",
    "\n",
    "We will extract `Candidate` objects of the `Spouse` type by identifying, for each `Sentence`, all pairs of ngrams (up to trigrams) that were tagged as people.  We do this with three objects:\n",
    "\n",
    "* A `ContextSpace` defines the \"space\" of all candidates we even potentially consider; in this case we use the `Ngrams` subclass, and look for all n-grams up to 3 words long\n",
    "\n",
    "* A `Matcher` heuristically filters the candidates we use.  In this case, we just use a pre-defined matcher which looks for all n-grams tagged by CoreNLP as \"PERSON\"\n",
    "\n",
    "* A `CandidateExtractor` combines this all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "from snorkel.matchers import PersonMatcher\n",
    "\n",
    "ngrams         = Ngrams(n_max=3)\n",
    "person_matcher = PersonMatcher(longest_match_only=True)\n",
    "cand_extractor = CandidateExtractor(Spouse, \n",
    "                                    [ngrams, ngrams], [person_matcher, person_matcher],\n",
    "                                    symmetric_relations=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading `Sentences` and splitting by `Document`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also _filter out_ sentences that mention at least five people:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def number_of_people(sentence):\n",
    "    active_sequence = False\n",
    "    count = 0\n",
    "    for tag in sentence.ner_tags:\n",
    "        if tag == 'PERSON' and not active_sequence:\n",
    "            active_sequence = True\n",
    "            count += 1\n",
    "        elif tag != 'PERSON' and active_sequence:\n",
    "            active_sequence = False\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll split the documents 90 / 5 / 5 into train / dev / test splits as is standard.  Note that here, we'll do this in non-random order to preserve the splits that we already labeled, and we'll reference them by 0 / 1 / 2 respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.models import Document\n",
    "\n",
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "ld   = len(docs)\n",
    "\n",
    "train_sents = set()\n",
    "dev_sents   = set()\n",
    "test_sents  = set()\n",
    "splits = (0.8, 0.9) if 'CI' in os.environ else (0.9, 0.95)\n",
    "for i,doc in enumerate(docs):\n",
    "    for s in doc.sentences:\n",
    "        if number_of_people(s) < 5:\n",
    "            if i < splits[0] * ld:\n",
    "                train_sents.add(s)\n",
    "            elif i < splits[1] * ld:\n",
    "                dev_sents.add(s)\n",
    "            else:\n",
    "                test_sents.add(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the `CandidateExtractor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the `CandidateExtractor` by calling extract with the contexts to extract from, a name for the `CandidateSet` that will contain the results, and the current session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%time cand_extractor.apply(train_sents, split=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we specified that these `Candidates` belong to the training set by specifying `split=0`; recall that we're referring to train / dev / test as splits 0 / 1 / 2.\n",
    "\n",
    "Note also that again, we could have specified a `parallelism` parameter to execute in parralel, if we had a non-SQLite database set up. Now let's get the candidates we just extracted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cands = session.query(Spouse).filter(Spouse.split == 0).all()\n",
    "print(\"Number of candidates:\", len(train_cands))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the `Viewer` to inspect candidates\n",
    "\n",
    "Next, we'll use the `Viewer` class--here, specifically, the `SentenceNgramViewer`--to inspect the data.\n",
    "\n",
    "It is important to note, our goal here is to **maximize the recall of true candidates** extracted, **not** to extract _only_ the correct candidates. Learning to distinguish true candidates from false candidates is covered in Tutorial 4.\n",
    "\n",
    "First, we instantiate the `Viewer` object, which groups the input `Candidate` objects by `Sentence`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "import os\n",
    "if 'CI' not in os.environ:\n",
    "    sv = SentenceNgramViewer(train_cands[:300], session)\n",
    "else:\n",
    "    sv = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we render the `Viewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can **navigate using the provided buttons**, or **using the keyboard (hover over buttons to see controls)**, highlight candidates (even if they overlap), and also **apply binary labels** (more on where to use this later!).  In particular, note that **the Viewer is synced dynamically with the notebook**, so that we can for example get the `Candidate` that is currently selected. Try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'CI' not in os.environ:\n",
    "    print(unicode(sv.get_selected()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traversing the _Context Hierarchy_\n",
    "\n",
    "As you have already probably observed, in Snorkel, all `Candidate`s are basically just tuples of `Context`-type objects--in this (and most) cases, `Span`s. Given a `Candidate`, we can easily access its `Context`s by the names you've given them, or as a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = train_cands[0]\n",
    "c.person1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.get_contexts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These `Context` objects are part of a hierarchy of `Context` objects in Snorkel.  In our case, this hierarchy consists of `Document`s, `Sentence`s, and `Span`s.  We can traverse this hierarchy by using the specific names--e.g., `doc.sentences`--or by using the generic `get_parent()` and `get_children()` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "span = c.get_contexts()[0]\n",
    "print(span)\n",
    "print(span.get_parent())\n",
    "print(span.get_parent().get_parent())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Span`s and `Sentence`s have special attributes which you can explore further in the documentation, and in tutorial section 4, when you will use some of these to write labeling functions.  For example, we can get the raw text span, the words, and other token types comprising a span:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(span.get_span())\n",
    "print(span.get_attrib_tokens())\n",
    "print(span.get_attrib_tokens('pos_tags'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeating for development and test corpora\n",
    "Finally, we will rerun the same operations for the other two news corpora: development and test. All we do for each is load in the `Corpus` object, collect the `Sentence` objects, and run them through the `CandidateExtractor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i, sents in enumerate([dev_sents, test_sents]):\n",
    "    cand_extractor.apply(sents, split=i+1)\n",
    "    print(\"Number of candidates:\", session.query(Spouse).filter(Spouse.split == i+1).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, in Part 3, we will annotate some candidates with labels so that we can evaluate performance."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
