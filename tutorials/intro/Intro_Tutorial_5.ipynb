{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro. to Snorkel: Extracting Spouse Relations from the News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part V: Training our End Extraction Model\n",
    "\n",
    "In this final section of the tutorial, we'll use the noisy training labels we generated in the last tutorial part to train our end extraction model.\n",
    "\n",
    "For this tutorial, we will be training a simple - but fairly effective - logistic regression model.  More generally, however, Snorkel plugs in with many ML libraries including [TensorFlow](https://www.tensorflow.org/), making it easy to use almost any state-of-the-art model as the end extractor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "# TO USE A DATABASE OTHER THAN SQLITE, USE THIS LINE\n",
    "# Note that this is necessary for parallel execution amongst other things...\n",
    "# os.environ['SNORKELDB'] = 'postgres:///snorkel-intro'\n",
    "\n",
    "import numpy as np\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat our definition of the `Spouse` `Candidate` subclass, and load the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "Spouse = candidate_subclass('Spouse', ['person1', 'person2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, we reload our **noise-aware training labels** (or _training marginals_) from the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_marginals\n",
    "train_marginals = load_marginals(session, split=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Automatically Creating Features\n",
    "\n",
    "First, we create features over the candidates in the training set. These features characterize the text and dependency path information related to the two person mentions in the candidate. **Note that we will define the set of features we use based on the training set here.**  Also note that this operation may take 5-10 minutes, so for large sets, parallelism should be used (by using a database like postgres and setting the `parallelism` keyword argument of `apply`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import FeatureAnnotator\n",
    "featurizer = FeatureAnnotator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 9min 29s, sys: 3.01 s, total: 9min 32s\n",
      "Wall time: 9min 40s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<4780x118064 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 281252 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time F_train = featurizer.apply(split=0)\n",
    "F_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we **apply the feature set we just got from the training set to the dev and test sets** by using `apply_existing`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 52.4 s, sys: 447 ms, total: 52.9 s\n",
      "Wall time: 53.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "F_dev  = featurizer.apply_existing(split=1)\n",
    "F_test = featurizer.apply_existing(split=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we've already computed the features, again we can just use the below step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "F_train = featurizer.load_matrix(session, split=0)\n",
    "F_dev   = featurizer.load_matrix(session, split=1)\n",
    "F_test  = featurizer.load_matrix(session, split=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training the Discriminative Model\n",
    "We use the training marginals to train a discriminative model that classifies each `Candidate` as a true or false mention. We'll use a random hyperparameter search, evaluated on the development set labels, to find the best hyperparameters for our model. To run a hyperparameter search, we need labels for a development set. If they aren't already available, we can manually create labels using the Viewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bradenhancock/anaconda/lib/python2.7/site-packages/matplotlib/__init__.py:1318: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning import SparseLogisticRegression\n",
    "disc_model = SparseLogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up and run the hyperparameter search, training our model with different hyperparamters and picking the best model configuration to keep. We'll set the random seed to maintain reproducibility.\n",
    "\n",
    "Note that we are fitting our model's parameters to the training set generated by our labeling functions, while we are picking hyperparamters with respect to score over the development set labels which we created by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized RandomSearch search of size 20. Search space size = 125.\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning.utils import MentionScorer\n",
    "from snorkel.learning import RandomSearch, ListParameter, RangeParameter\n",
    "\n",
    "# Searching over learning rate\n",
    "rate_param = RangeParameter('lr', 1e-6, 1e-2, step=1, log_base=10)\n",
    "l1_param  = RangeParameter('l1_penalty', 1e-6, 1e-2, step=1, log_base=10)\n",
    "l2_param  = RangeParameter('l2_penalty', 1e-6, 1e-2, step=1, log_base=10)\n",
    "\n",
    "searcher = RandomSearch(session, disc_model, F_train, train_marginals, [rate_param, l1_param, l2_param], n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll load in our dev set labels. We will pick the optimal result from the hyperparameter search by testing against these labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the hyperparameter search / train the end extraction model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "[1] Testing lr = 1.00e-02, l1_penalty = 1.00e-03, l2_penalty = 1.00e-04\n",
      "============================================================\n",
      "[SparseLR] lr=0.01 l1=0.001 l2=0.0001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=100\n",
      "[SparseLR] Epoch 0 (0.11s)\tAvg. loss=0.673512\tNNZ=118050\n",
      "[SparseLR] Epoch 25 (0.73s)\tAvg. loss=0.558851\tNNZ=118045\n",
      "[SparseLR] Epoch 49 (1.40s)\tAvg. loss=0.555625\tNNZ=117906\n",
      "[SparseLR] Training done (1.40s)\n",
      "[SparseLR] Model saved. To load, use name\n",
      "\t\tSparseLR_0\n",
      "============================================================\n",
      "[2] Testing lr = 1.00e-04, l1_penalty = 1.00e-06, l2_penalty = 1.00e-03\n",
      "============================================================\n",
      "[SparseLR] lr=0.0001 l1=1e-06 l2=0.001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=100\n",
      "[SparseLR] Epoch 0 (0.16s)\tAvg. loss=0.692559\tNNZ=118064\n",
      "[SparseLR] Epoch 25 (0.84s)\tAvg. loss=0.662954\tNNZ=118064\n",
      "[SparseLR] Epoch 49 (1.41s)\tAvg. loss=0.647119\tNNZ=118064\n",
      "[SparseLR] Training done (1.41s)\n",
      "============================================================\n",
      "[3] Testing lr = 1.00e-03, l1_penalty = 1.00e-05, l2_penalty = 1.00e-05\n",
      "============================================================\n",
      "[SparseLR] lr=0.001 l1=1e-05 l2=1e-05\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=100\n",
      "[SparseLR] Epoch 0 (0.10s)\tAvg. loss=0.687742\tNNZ=118064\n",
      "[SparseLR] Epoch 25 (0.78s)\tAvg. loss=0.602511\tNNZ=118064\n",
      "[SparseLR] Epoch 49 (1.37s)\tAvg. loss=0.582559\tNNZ=118064\n",
      "[SparseLR] Training done (1.37s)\n",
      "============================================================\n",
      "[4] Testing lr = 1.00e-03, l1_penalty = 1.00e-06, l2_penalty = 1.00e-03\n",
      "============================================================\n",
      "[SparseLR] lr=0.001 l1=1e-06 l2=0.001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=100\n",
      "[SparseLR] Epoch 0 (0.11s)\tAvg. loss=0.690027\tNNZ=118064\n",
      "[SparseLR] Epoch 25 (0.70s)\tAvg. loss=0.603158\tNNZ=118064\n",
      "[SparseLR] Epoch 49 (1.35s)\tAvg. loss=0.583749\tNNZ=118064\n",
      "[SparseLR] Training done (1.35s)\n",
      "============================================================\n",
      "[5] Testing lr = 1.00e-02, l1_penalty = 1.00e-04, l2_penalty = 1.00e-05\n",
      "============================================================\n",
      "[SparseLR] lr=0.01 l1=0.0001 l2=1e-05\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=100\n",
      "[SparseLR] Epoch 0 (0.11s)\tAvg. loss=0.670348\tNNZ=118063\n",
      "[SparseLR] Epoch 25 (0.73s)\tAvg. loss=0.555977\tNNZ=118064\n",
      "[SparseLR] Epoch 49 (1.33s)\tAvg. loss=0.553150\tNNZ=118062\n",
      "[SparseLR] Training done (1.33s)\n",
      "[SparseLR] Model saved. To load, use name\n",
      "\t\tSparseLR_4\n",
      "============================================================\n",
      "[6] Testing lr = 1.00e-06, l1_penalty = 1.00e-03, l2_penalty = 1.00e-05\n",
      "============================================================\n",
      "[SparseLR] lr=1e-06 l1=0.001 l2=1e-05\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=100\n",
      "[SparseLR] Epoch 0 (0.12s)\tAvg. loss=0.693336\tNNZ=118064\n",
      "[SparseLR] Epoch 25 (0.73s)\tAvg. loss=0.693077\tNNZ=118064\n",
      "[SparseLR] Epoch 49 (1.34s)\tAvg. loss=0.692543\tNNZ=118064\n",
      "[SparseLR] Training done (1.34s)\n",
      "============================================================\n",
      "[7] Testing lr = 1.00e-06, l1_penalty = 1.00e-03, l2_penalty = 1.00e-02\n",
      "============================================================\n",
      "[SparseLR] lr=1e-06 l1=0.001 l2=0.01\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=100\n",
      "[SparseLR] Epoch 0 (0.12s)\tAvg. loss=0.696536\tNNZ=118064\n",
      "[SparseLR] Epoch 25 (0.70s)\tAvg. loss=0.695912\tNNZ=118064\n",
      "[SparseLR] Epoch 49 (1.38s)\tAvg. loss=0.695319\tNNZ=118064\n",
      "[SparseLR] Training done (1.38s)\n",
      "============================================================\n",
      "[8] Testing lr = 1.00e-02, l1_penalty = 1.00e-05, l2_penalty = 1.00e-02\n",
      "============================================================\n",
      "[SparseLR] lr=0.01 l1=1e-05 l2=0.01\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=100\n",
      "[SparseLR] Epoch 0 (0.14s)\tAvg. loss=0.669412\tNNZ=118064\n",
      "[SparseLR] Epoch 25 (0.91s)\tAvg. loss=0.554229\tNNZ=118064\n",
      "[SparseLR] Epoch 49 (1.59s)\tAvg. loss=0.551175\tNNZ=118064\n",
      "[SparseLR] Training done (1.59s)\n",
      "============================================================\n",
      "[9] Testing lr = 1.00e-04, l1_penalty = 1.00e-02, l2_penalty = 1.00e-06\n",
      "============================================================\n",
      "[SparseLR] lr=0.0001 l1=0.01 l2=1e-06\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=100\n",
      "[SparseLR] Epoch 0 (0.15s)\tAvg. loss=0.693099\tNNZ=118064\n",
      "[SparseLR] Epoch 25 (0.91s)\tAvg. loss=0.664594\tNNZ=118060\n",
      "[SparseLR] Epoch 49 (1.52s)\tAvg. loss=0.649825\tNNZ=118062\n",
      "[SparseLR] Training done (1.52s)\n",
      "============================================================\n",
      "[10] Testing lr = 1.00e-06, l1_penalty = 1.00e-05, l2_penalty = 1.00e-03\n",
      "============================================================\n",
      "[SparseLR] lr=1e-06 l1=1e-05 l2=0.001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=100\n",
      "[SparseLR] Epoch 0 (0.15s)\tAvg. loss=0.693029\tNNZ=118064\n",
      "[SparseLR] Epoch 25 (0.79s)\tAvg. loss=0.692936\tNNZ=118064\n",
      "[SparseLR] Epoch 49 (1.43s)\tAvg. loss=0.692368\tNNZ=118064\n",
      "[SparseLR] Training done (1.43s)\n",
      "============================================================\n",
      "[11] Testing lr = 1.00e-04, l1_penalty = 1.00e-02, l2_penalty = 1.00e-04\n",
      "============================================================\n",
      "[SparseLR] lr=0.0001 l1=0.01 l2=0.0001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=100\n",
      "[SparseLR] Epoch 0 (0.15s)\tAvg. loss=0.689958\tNNZ=118061\n",
      "[SparseLR] Epoch 25 (0.79s)\tAvg. loss=0.660930\tNNZ=118064\n",
      "[SparseLR] Epoch 49 (1.46s)\tAvg. loss=0.645858\tNNZ=118063\n",
      "[SparseLR] Training done (1.46s)\n",
      "============================================================\n",
      "[12] Testing lr = 1.00e-03, l1_penalty = 1.00e-06, l2_penalty = 1.00e-03\n",
      "============================================================\n",
      "[SparseLR] lr=0.001 l1=1e-06 l2=0.001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=100\n",
      "[SparseLR] Epoch 0 (0.16s)\tAvg. loss=0.688393\tNNZ=118064\n",
      "[SparseLR] Epoch 25 (0.81s)\tAvg. loss=0.599652\tNNZ=118064\n",
      "[SparseLR] Epoch 49 (1.53s)\tAvg. loss=0.580418\tNNZ=118064\n",
      "[SparseLR] Training done (1.53s)\n",
      "============================================================\n",
      "[13] Testing lr = 1.00e-04, l1_penalty = 1.00e-04, l2_penalty = 1.00e-05\n",
      "============================================================\n",
      "[SparseLR] lr=0.0001 l1=0.0001 l2=1e-05\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=100\n",
      "[SparseLR] Epoch 0 (0.16s)\tAvg. loss=0.694530\tNNZ=118064\n",
      "[SparseLR] Epoch 25 (0.77s)\tAvg. loss=0.665219\tNNZ=118064\n",
      "[SparseLR] Epoch 49 (1.39s)\tAvg. loss=0.649795\tNNZ=118064\n",
      "[SparseLR] Training done (1.39s)\n",
      "============================================================\n",
      "[14] Testing lr = 1.00e-03, l1_penalty = 1.00e-05, l2_penalty = 1.00e-04\n",
      "============================================================\n",
      "[SparseLR] lr=0.001 l1=1e-05 l2=0.0001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=100\n",
      "[SparseLR] Epoch 0 (0.19s)\tAvg. loss=0.687427\tNNZ=118064\n",
      "[SparseLR] Epoch 25 (0.83s)\tAvg. loss=0.597539\tNNZ=118064\n",
      "[SparseLR] Epoch 49 (1.59s)\tAvg. loss=0.577618\tNNZ=118064\n",
      "[SparseLR] Training done (1.59s)\n",
      "============================================================\n",
      "[15] Testing lr = 1.00e-04, l1_penalty = 1.00e-02, l2_penalty = 1.00e-05\n",
      "============================================================\n",
      "[SparseLR] lr=0.0001 l1=0.01 l2=1e-05\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=100\n",
      "[SparseLR] Epoch 0 (0.18s)\tAvg. loss=0.691328\tNNZ=118062\n",
      "[SparseLR] Epoch 25 (0.84s)\tAvg. loss=0.663883\tNNZ=118062\n",
      "[SparseLR] Epoch 49 (1.46s)\tAvg. loss=0.649699\tNNZ=118062\n",
      "[SparseLR] Training done (1.46s)\n",
      "============================================================\n",
      "[16] Testing lr = 1.00e-05, l1_penalty = 1.00e-06, l2_penalty = 1.00e-04\n",
      "============================================================\n",
      "[SparseLR] lr=1e-05 l1=1e-06 l2=0.0001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=100\n",
      "[SparseLR] Epoch 0 (0.18s)\tAvg. loss=0.693722\tNNZ=118064\n",
      "[SparseLR] Epoch 25 (0.89s)\tAvg. loss=0.689718\tNNZ=118064\n",
      "[SparseLR] Epoch 49 (1.66s)\tAvg. loss=0.686869\tNNZ=118064\n",
      "[SparseLR] Training done (1.66s)\n",
      "============================================================\n",
      "[17] Testing lr = 1.00e-06, l1_penalty = 1.00e-02, l2_penalty = 1.00e-03\n",
      "============================================================\n",
      "[SparseLR] lr=1e-06 l1=0.01 l2=0.001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=100\n",
      "[SparseLR] Epoch 0 (0.20s)\tAvg. loss=0.693865\tNNZ=118064\n",
      "[SparseLR] Epoch 25 (0.82s)\tAvg. loss=0.693292\tNNZ=118064\n",
      "[SparseLR] Epoch 49 (1.42s)\tAvg. loss=0.692748\tNNZ=118064\n",
      "[SparseLR] Training done (1.42s)\n",
      "============================================================\n",
      "[18] Testing lr = 1.00e-02, l1_penalty = 1.00e-05, l2_penalty = 1.00e-06\n",
      "============================================================\n",
      "[SparseLR] lr=0.01 l1=1e-05 l2=1e-06\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=100\n",
      "[SparseLR] Epoch 0 (0.19s)\tAvg. loss=0.672198\tNNZ=118064\n",
      "[SparseLR] Epoch 25 (0.78s)\tAvg. loss=0.555197\tNNZ=118062\n",
      "[SparseLR] Epoch 49 (1.40s)\tAvg. loss=0.552245\tNNZ=118064\n",
      "[SparseLR] Training done (1.40s)\n",
      "============================================================\n",
      "[19] Testing lr = 1.00e-05, l1_penalty = 1.00e-02, l2_penalty = 1.00e-06\n",
      "============================================================\n",
      "[SparseLR] lr=1e-05 l1=0.01 l2=1e-06\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=100\n",
      "[SparseLR] Epoch 0 (0.19s)\tAvg. loss=0.692512\tNNZ=118064\n",
      "[SparseLR] Epoch 25 (0.82s)\tAvg. loss=0.686790\tNNZ=118064\n",
      "[SparseLR] Epoch 49 (1.43s)\tAvg. loss=0.683428\tNNZ=118063\n",
      "[SparseLR] Training done (1.43s)\n",
      "============================================================\n",
      "[20] Testing lr = 1.00e-02, l1_penalty = 1.00e-05, l2_penalty = 1.00e-04\n",
      "============================================================\n",
      "[SparseLR] lr=0.01 l1=1e-05 l2=0.0001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=100\n",
      "[SparseLR] Epoch 0 (0.21s)\tAvg. loss=0.670169\tNNZ=118064\n",
      "[SparseLR] Epoch 25 (0.84s)\tAvg. loss=0.556515\tNNZ=118064\n",
      "[SparseLR] Epoch 49 (1.52s)\tAvg. loss=0.553366\tNNZ=118064\n",
      "[SparseLR] Training done (1.52s)\n",
      "[SparseLR] Loaded model <SparseLR_4>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>l1_penalty</th>\n",
       "      <th>l2_penalty</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>Rec.</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.044248</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.074074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.019048</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.035714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.025478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          lr  l1_penalty  l2_penalty     Prec.      Rec.        F1\n",
       "4   0.010000    0.000100    0.000010  0.600000  0.428571  0.500000\n",
       "0   0.010000    0.001000    0.000100  1.000000  0.285714  0.444444\n",
       "17  0.010000    0.000010    0.000001  1.000000  0.285714  0.444444\n",
       "7   0.010000    0.000010    0.010000  0.666667  0.285714  0.400000\n",
       "19  0.010000    0.000010    0.000100  0.666667  0.285714  0.400000\n",
       "18  0.000010    0.010000    0.000001  0.085714  0.428571  0.142857\n",
       "16  0.000001    0.010000    0.001000  0.044248  0.714286  0.083333\n",
       "15  0.000010    0.000001    0.000100  0.042553  0.285714  0.074074\n",
       "9   0.000001    0.000010    0.001000  0.019048  0.285714  0.035714\n",
       "6   0.000001    0.001000    0.010000  0.013333  0.285714  0.025478\n",
       "5   0.000001    0.001000    0.000010  0.000000  0.000000  0.000000\n",
       "3   0.001000    0.000001    0.001000  0.000000  0.000000  0.000000\n",
       "8   0.000100    0.010000    0.000001  0.000000  0.000000  0.000000\n",
       "1   0.000100    0.000001    0.001000  0.000000  0.000000  0.000000\n",
       "11  0.001000    0.000001    0.001000  0.000000  0.000000  0.000000\n",
       "12  0.000100    0.000100    0.000010  0.000000  0.000000  0.000000\n",
       "13  0.001000    0.000010    0.000100  0.000000  0.000000  0.000000\n",
       "14  0.000100    0.010000    0.000010  0.000000  0.000000  0.000000\n",
       "2   0.001000    0.000010    0.000010  0.000000  0.000000  0.000000\n",
       "10  0.000100    0.010000    0.000100  0.000000  0.000000  0.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1701)\n",
    "searcher.fit(F_dev, L_gold_dev, n_epochs=50, rebalance=True, print_freq=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note that to train a model without tuning any hyperparameters (at your own risk) just use the `train` method of the discriminative model. For instance, to train with 20 epochs and a learning rate of 0.001, you could run:_\n",
    "```\n",
    "disc_model.train(F_train, train_marginals, n_epochs=20, lr=0.001)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluating on the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last section of the tutorial, we'll get the score we've been after: the performance of the extraction model on the blind test set (`split` 2). First, we load the test set labels and gold candidates we made in Part III."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we score using the discriminative model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.429\n",
      "Neg. class accuracy: 0.993\n",
      "Precision            0.6\n",
      "Recall               0.429\n",
      "F1                   0.5\n",
      "----------------------------------------\n",
      "TP: 3 | FP: 2 | TN: 270 | FN: 4\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, _, _, _ = disc_model.score(session, F_test, L_gold_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if this is the final test set that you will be reporting final numbers on, to avoid biasing results you should not inspect results.  However you can run the model on your _development set_ and, as we did in the previous part with the generative labeling function model, inspect examples to do error analysis.\n",
    "\n",
    "##### More importantly, you've now completed the introduction to Snorkel! Give yourself a pat on the back!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
