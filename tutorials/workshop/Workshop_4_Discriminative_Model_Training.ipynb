{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"imgs/logo.jpg\" width=\"50px\" style=\"margin-right:10px\">\n",
    "\n",
    "# Snorkel Workshop: Extracting Spouse Relations <br> from the News\n",
    "## Part 4: Training our End Extraction Model\n",
    "\n",
    "In this final section of the tutorial, we'll use the noisy training labels we generated in the last tutorial part to train our end machine learning model.\n",
    "\n",
    "For this tutorial, we will be training a fairly effective deep learning model. More generally, however, Snorkel plugs in with many ML libraries, making it easy to use almost any state-of-the-art model as the end model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload \n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from snorkel.model.utils import MetalDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Loading Candidates and Gold Labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('train_data.pkl', 'rb') as f:\n",
    "    test_df = pickle.load(f)\n",
    "    test_labels = pickle.load(f)\n",
    "\n",
    "with open('dev_data.pkl', 'rb') as f:\n",
    "    dev_df = pickle.load(f)\n",
    "    dev_labels = pickle.load(f)\n",
    "    \n",
    "with open('train_data.pkl', 'rb') as f:\n",
    "    train_df = pickle.load(f)\n",
    "\n",
    "with open('train_proba.pkl', 'rb') as f:\n",
    "    train_marginals = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Training a _Long Short-term Memory_ (LSTM) Neural Network\n",
    "\n",
    "[LSTMs](https://en.wikipedia.org/wiki/Long_short-term_memory) can acheive state-of-the-art performance on many text classification tasks. We'll train a simple LSTM model below. \n",
    "\n",
    "In deep learning, hyperparameter tuning is very important and computationally expensive step in training models. For purposes of this tutorial, we have a pre-trained model using the training labels generated from the previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing for LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we prepare the input to our LSTM by adding *markers* to the beginning and end of the person mentions so the LSTM knows which two persons in the sentence we want to learn a relation for. We then featurize the tokens it using a standard vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import EmbeddingFeaturizer\n",
    "from utils import mark_entities\n",
    "\n",
    "markers = ['[[BEGIN0]]','[[END0]]','[[BEGIN1]]','[[END1]]']\n",
    "featurizer = EmbeddingFeaturizer(markers=markers)\n",
    "\n",
    "def convert_to_lstm_input(data):\n",
    "    X = []\n",
    "    #mark candidates with markers\n",
    "    for i in range(len(data)):\n",
    "        cand = data.loc[i]\n",
    "        marked_tokens = mark_entities(\n",
    "                    cand.tokens,\n",
    "                    positions=[cand.person1_word_idx, cand.person2_word_idx],\n",
    "                    markers=markers)\n",
    "        X.append(marked_tokens)\n",
    "        \n",
    "    #featurize string tokens tokens\n",
    "    featurize_X = featurizer.fit_transform(X)\n",
    "    return featurize_X\n",
    "\n",
    "train_X_tensor = convert_to_lstm_input(train_df)\n",
    "dev_X_tensor = convert_to_lstm_input(dev_df)\n",
    "test_X_tensor = convert_to_lstm_input(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import upgrade_dataloaders\n",
    "\n",
    "datasets = []\n",
    "datasets.append(MetalDataset(train_X_tensor, torch.LongTensor(train_marginals[:,0]))) #TODO: check \n",
    "datasets.append(MetalDataset(dev_X_tensor, torch.LongTensor(dev_labels+1.)))\n",
    "datasets.append(MetalDataset(test_X_tensor, torch.LongTensor(test_labels+1.)))\n",
    "\n",
    "dataloaders = []\n",
    "for dataset, split in zip(datasets, [\"train\", \"valid\", \"test\"]):\n",
    "    dataloader = DataLoader(dataset)\n",
    "    dataloader.split = split\n",
    "    dataloaders.append(dataloader)\n",
    "    \n",
    "dataloaders = upgrade_dataloaders(dataloaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training LSTM Model\n",
    "For purposes of this tutorial, we have saved a pre-trained model that was trained using probabilistic labels generated in the previous notebook. \n",
    "\n",
    "We define our model here and load the pretrained weights before evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleModel(name=SimpleModel)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from snorkel.mtl.simple_model import SimpleModel\n",
    "from utils import LSTMModule, EmbeddingsEncoder\n",
    "\n",
    "MAX_INT = train_X_tensor.max()\n",
    "embed_size = 4\n",
    "hidden_size = 5\n",
    "\n",
    "lstm_module = LSTMModule(\n",
    "    embed_size,\n",
    "    hidden_size,\n",
    "    bidirectional=False,\n",
    "    verbose=False,\n",
    "    lstm_reduction=\"attention\",\n",
    "    encoder_class=EmbeddingsEncoder,\n",
    "    encoder_kwargs={\"vocab_size\": MAX_INT + 1},\n",
    ")\n",
    "\n",
    "model = SimpleModel(\n",
    "    modules=[\n",
    "    lstm_module,\n",
    "    nn.Linear(lstm_module.output_dim,1)],\n",
    "    metrics = ['accuracy', 'f1', 'precision','recall'])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load and Score Pre-Trained Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Set Scores\n",
      "{'task/data_valid/valid/accuracy': 0.3624161073825503, 'task/data_valid/valid/f1': 0.5320197044334976, 'task/data_valid/valid/precision': 0.3624161073825503, 'task/data_valid/valid/recall': 1.0}\n"
     ]
    }
   ],
   "source": [
    "model.load('./trained_spouse_model')\n",
    "\n",
    "print(\"Dev Set Scores\")\n",
    "scores = model.score(dataloaders[1])\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: This takes > 30 mins to Run on a CPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train SimpleModel\n",
    "# from snorkel.mtl.trainer import Trainer\n",
    "# trainer = Trainer(progress_bar=True, n_epochs=5)\n",
    "# trainer.train_model(model, dataloaders)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
